### LightRAG with Azure OpenAI and Multimodal Support Configuration
### This is a sample .env file configured for Azure OpenAI with multimodal capabilities

###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='Azure OpenAI Multimodal Graph KB'
WEBUI_DESCRIPTION="LightRAG with Azure OpenAI and Multimodal Support"
# WORKERS=2
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Directory Configuration
# INPUT_DIR=/app/data/inputs
# WORKING_DIR=/app/data/rag_storage
# MULTIMODAL_OUTPUT_DIR=/app/data/multimodal_output

### Tiktoken cache directory
# TIKTOKEN_CACHE_DIR=/app/data/tiktoken

### Max nodes return from graph retrieval in webui
# MAX_GRAPH_NODES=1000

### Logging Configuration
# LOG_LEVEL=INFO
# VERBOSE=False

#####################################
### Login and API-Key Configuration
#####################################
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here

######################################################################################
### Query Configuration
###
### How to control the context length sent to LLM:
###    MAX_ENTITY_TOKENS + MAX_RELATION_TOKENS < MAX_TOTAL_TOKENS
###    Chunk_Tokens = MAX_TOTAL_TOKENS - Actual_Entity_Tokens - Actual_Relation_Tokens
######################################################################################
ENABLE_LLM_CACHE=true
# COSINE_THRESHOLD=0.2
### Number of entities or relations retrieved from KG
TOP_K=40
### Maximum number of chunks for naive vector search
CHUNK_TOP_K=10
### Control the actual entities sent to LLM
MAX_ENTITY_TOKENS=10000
### Control the actual relations sent to LLM
MAX_RELATION_TOKENS=10000
### Control the maximum tokens sent to LLM (include entities, relations and chunks)
MAX_TOTAL_TOKENS=30000
### Maximum number of related chunks per source entity or relation
# RELATED_CHUNK_NUMBER=5

### Reranker configuration
ENABLE_RERANK=true
### Minimum rerank score for document chunk exclusion
MIN_RERANK_SCORE=0.0
### Rerank model configuration (optional with Azure OpenAI)
# RERANK_MODEL=jina-reranker-v2-base-multilingual
# RERANK_BINDING_HOST=https://api.jina.ai/v1/rerank
# RERANK_BINDING_API_KEY=your_rerank_api_key_here

########################################
### Document Processing Configuration
########################################
SUMMARY_LANGUAGE=English
ENABLE_LLM_CACHE_FOR_EXTRACT=true
### Chunk size for document splitting, 500~1500 is recommended
CHUNK_SIZE=1200
CHUNK_OVERLAP_SIZE=100
### Entity and relation summarization configuration
FORCE_LLM_SUMMARY_ON_MERGE=4
MAX_TOKENS=10000
### Maximum number of entity extraction attempts for ambiguous content
MAX_GLEANING=1

###############################
### Concurrency Configuration
###############################
### Max concurrency requests of LLM (for both query and document processing)
MAX_ASYNC=4
### Number of parallel processing documents (between 2~10, MAX_ASYNC/3 is recommended)
MAX_PARALLEL_INSERT=2
### Max concurrency requests for Embedding
EMBEDDING_FUNC_MAX_ASYNC=8
### Number of chunks sent to Embedding in single request
EMBEDDING_BATCH_NUM=10

###########################################################
### Azure OpenAI LLM Configuration
###########################################################
LLM_BINDING=azure_openai
LLM_MODEL=gpt-4o
TEMPERATURE=0.8

### Azure OpenAI Configuration - Replace with your actual values
AZURE_OPENAI_API_KEY=a34d0b805ee74799a75e54cb578d1700
AZURE_OPENAI_ENDPOINT=https://argenti-aitest.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-08-01-preview
AZURE_OPENAI_DEPLOYMENT=gpt-4o

### Timeout configuration
TIMEOUT=240

####################################################################################
### Azure OpenAI Embedding Configuration
### WARNING: Should not be changed after the first file is processed
####################################################################################
EMBEDDING_BINDING=azure_openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_DIM=3072

### Azure Embedding Configuration - Replace with your actual values
AZURE_EMBEDDING_API_KEY=a34d0b805ee74799a75e54cb578d1700
AZURE_EMBEDDING_ENDPOINT=https://argenti-aitest.openai.azure.com/
AZURE_EMBEDDING_API_VERSION=2023-05-15
AZURE_EMBEDDING_DEPLOYMENT=text-embedding-3-large

########################################
### Multimodal Configuration
########################################
### Enable multimodal document processing
ENABLE_MULTIMODAL=true
### Multimodal output directory for processed content
MULTIMODAL_OUTPUT_DIR=/app/data/multimodal_output
### Vision model configuration for image processing (uses same Azure OpenAI endpoint)
VISION_MODEL=gpt-4o
AZURE_VISION_DEPLOYMENT=gpt-4o
### Image processing settings
ENABLE_IMAGE_PROCESSING=true
ENABLE_OFFICE_PROCESSING=true
ENABLE_PDF_PROCESSING=true
### Custom prompts for multimodal processing
IMAGE_DESCRIPTION_PROMPT="Describe this image in detail, focusing on any text, data, charts, or important visual elements."
TABLE_EXTRACTION_PROMPT="Extract and describe the structure and content of any tables in this document."

####################################################################
### WORKSPACE setting workspace name for all storage types
### in the purpose of isolating data from LightRAG instances.
### Valid workspace name constraints: a-z, A-Z, 0-9, and _
####################################################################
WORKSPACE=azure_multimodal

############################
### Data Storage Selection
############################
### PostgreSQL Storage (Recommended for Docker deployment)
LIGHTRAG_KV_STORAGE=PGKVStorage
LIGHTRAG_DOC_STATUS_STORAGE=PGDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=Neo4JStorage
LIGHTRAG_VECTOR_STORAGE=PGVectorStorage

### PostgreSQL Configuration (Docker setup)
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=lightrag_user
POSTGRES_PASSWORD=lightrag_password
POSTGRES_DATABASE=lightrag
POSTGRES_MAX_CONNECTIONS=12

### PostgreSQL Vector Storage Configuration
### Vector storage type: HNSW, IVFFlat
POSTGRES_VECTOR_INDEX_TYPE=HNSW
POSTGRES_HNSW_M=16
POSTGRES_HNSW_EF=200
POSTGRES_IVFFLAT_LISTS=100

### Alternative Storage Options (uncomment if needed)
### Default storage (for small scale deployment)
# LIGHTRAG_KV_STORAGE=JsonKVStorage
# LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
# LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
# LIGHTRAG_VECTOR_STORAGE=NanoVectorDBStorage

### Production storage options
# LIGHTRAG_VECTOR_STORAGE=MilvusVectorDBStorage
# LIGHTRAG_VECTOR_STORAGE=QdrantVectorDBStorage
# LIGHTRAG_GRAPH_STORAGE=Neo4JStorage


### Neo4j Configuration
NEO4J_URI=bolt://neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_MAX_CONNECTION_POOL_SIZE=100
NEO4J_CONNECTION_TIMEOUT=30
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=30
NEO4J_MAX_TRANSACTION_RETRY_TIME=30
NEO4J_MAX_CONNECTION_LIFETIME=300
NEO4J_LIVENESS_CHECK_TIMEOUT=30
NEO4J_KEEP_ALIVE=true
